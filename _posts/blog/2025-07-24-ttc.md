---
layout: post
title:  "Test Time Compute"
date:   2025-07-24
categories: machine_learning
---

# Test-Time Compute: "Think Step-by-Step"

It has been eight years since the original Transformer architecture was proposed, and the world of large language models continues to evolve at a breathtaking pace. At first glance, the underlying architecture of today's frontier models might seem remarkably similar to their ancestors. We've seen advancements like rotational position embeddings and more efficient attention mechanisms, but the fundamental building blocks often remain the same.

However, a different kind of evolution is taking place. It’s not just about building bigger models trained on more data; it's about how we *use* these models. We are moving from an era where models give us instantaneous, "System 1" answers to one where they can engage in deliberate, "System 2" reasoning. This shift is powered by a concept that is rapidly becoming a new frontier in AI: **Test-Time Compute**.

Test-Time Compute (TTC) refers to the computational resources used during the inference phase—the moment you ask a model a question. Instead of just performing a single forward pass and returning an answer, we can allow the model to "think longer," using additional computation to explore, refine, and verify its response. As returns from scaling pre-training compute and data begin to diminish, many believe that sophisticated use of test-time compute will be the key driver of future performance gains.

In this article, we'll explore this exciting frontier, focusing on the family of techniques that started it all: Chain-of-Thought and its powerful descendants. We'll journey from simple reasoning chains to complex, dynamic graphs of thought, tracing the path towards models that don't just answer, but *reason*.

### 1. The Spark of Reasoning: Chain-of-Thought (CoT)

The standard autoregressive process of a language model—generating a response token by token, from left to right—is inherently myopic. For tasks requiring multi-step logic, this can be a significant handicap. A model might generate a plausible-sounding answer that is factually or logically incorrect because it never had a chance to lay out the intermediate steps.

**Chain-of-Thought (CoT) prompting** was the first major breakthrough to address this. The core idea is brilliantly simple: instead of just asking for the final answer, you prompt the model to show its work. By generating intermediate reasoning steps, the model can decompose a complex problem into a series of simpler ones, dramatically improving its ability to arrive at the correct solution.

CoT prompting primarily comes in two flavors:

*   **Few-Shot CoT:** This involves providing the model with a few examples in the prompt that demonstrate the step-by-step reasoning process. The model then learns to mimic this pattern for a new, unseen problem.

    *Figure 1: A comparison between standard few-shot prompting and Chain-of-Thought prompting. By showing its work, the model is guided toward the correct reasoning process.*
    
    **Standard Prompt:**
    > Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?
    > A: 11
    >
    > Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?
    > A: 6

    **Chain-of-Thought Prompt:**
    > Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?
    > A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.
    >
    > Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?
    > A: There are 15 trees to start. There are 21 trees later. The difference is the number of trees planted. 21 - 15 = 6. The answer is 6.

*   **Zero-Shot CoT:** Perhaps more surprisingly, researchers discovered that you don't always need to provide explicit examples. For sufficiently large models, simply appending the magic words, "**Let's think step by step**," is enough to elicit a reasoning chain and improve performance.

While revolutionary, CoT is most effective in very large models (typically those with over 100 billion parameters) and still relies on a single, greedily decoded reasoning path, which may not always be correct.

### 2. Finding Strength in Numbers: Self-Consistency

The initial implementation of CoT is like asking a single student to solve a problem on a whiteboard. If they make a mistake early on, the entire answer will be wrong. A more robust approach would be to ask multiple students to solve the problem independently and then see which answer is the most common.

This is the intuition behind **Self-Consistency**. Instead of just generating the single most likely reasoning path (greedy decoding), self-consistency samples a diverse set of possible reasoning paths from the model. It then marginalizes out these different "ways of thinking" and chooses the final answer that appears most consistently across the different paths.

<br>
*Figure 2: An illustration of Self-Consistency. Instead of taking the single, greedy path, multiple reasoning paths are sampled. The final answer is determined by a majority vote, which is more robust to individual errors.*
<br>

This simple yet powerful decoding strategy significantly boosts the performance of CoT prompting. It leverages the idea that for many complex problems, there are multiple valid paths to the correct solution. By exploring several of these, the model has a much higher chance of finding the right answer, even if some of its individual reasoning attempts are flawed.

### 3. From a Chain to a Tree: Deliberate Exploration

Linear reasoning, even with multiple attempts, doesn't capture the full picture of complex problem-solving. Human thought is often non-linear. We explore different avenues, hit dead ends, backtrack, and pursue more promising paths. A single chain, or even a bundle of independent chains, cannot model this kind of deliberate exploration.

This limitation led to the development of the **Tree of Thoughts (ToT)** framework. ToT generalizes CoT by structuring the reasoning process as a tree, where each node is a "thought"—a coherent unit of text that represents an intermediate step in problem-solving. This allows the model to actively explore different reasoning branches.

The ToT framework involves several key components:

1.  **Thought Generation:** At each node in the tree, the model generates several potential next thoughts or steps.
2.  **State Evaluation:** The model itself is used as a heuristic to evaluate the "promise" of each potential path. It can assess whether a particular thought is likely to lead to a solution or a dead end.
3.  **Search Algorithm:** A search algorithm, such as breadth-first search (BFS) or depth-first search (DFS), is used to navigate the tree. This enables the system to systematically explore possibilities, look ahead, and even backtrack when a path seems unpromising.

For example, in the "Game of 24," where the goal is to use four numbers and basic arithmetic to get 24, ToT allows the model to explore different combinations of operations in parallel, discard failures, and pursue the most viable routes, dramatically outperforming standard CoT.

<br>
*Figure 3: A conceptual comparison of CoT and ToT. CoT follows a single path, while ToT explores a tree of possibilities, allowing for evaluation and backtracking.*
<br>

### 4. The Networked Mind: Graph of Thoughts (GoT)

A tree structure is a major step up from a linear chain, but human thought can be even more interconnected. We merge ideas from different lines of reasoning, create feedback loops to refine our thoughts, and build a complex network of understanding.

The **Graph of Thoughts (GoT)** framework extends the exploration paradigm even further by modeling the reasoning process as an arbitrary graph. In this model, thoughts are vertices, and the dependencies between them are edges. This unlocks a new level of sophistication in machine reasoning.

The graph structure enables novel transformations not possible with chains or trees:

*   **Aggregation:** Multiple thoughts (nodes) can be merged into a single, synergistic new thought. For instance, the model could synthesize key points from several different lines of argument into a single, more robust conclusion.
*   **Refinement:** A thought can be enhanced through a feedback loop (an edge pointing back to the same node), allowing for iterative self-correction and improvement.

GoT represents a shift from exploring predefined paths to dynamically constructing a network of reasoning, bringing us closer to modeling the complex, non-linear nature of human cognition.

### Conclusion: Reasoning as a First-Class Citizen

The journey from Chain-of-Thought to Graph-of-Thoughts illustrates a clear and powerful trend: we are building systems that treat reasoning not as an emergent property of next-token prediction, but as a deliberate, structured, and resource-intensive process. These techniques can be viewed through the even broader lens of **Reasoning as Planning (RAP)**, where an LLM acts as both an agent exploring a problem space and a world model predicting the outcome of its actions, often guided by formal search algorithms like MCTS.

The implication is profound. The future of AI performance may not lie solely in scaling model parameters at training time, but in scaling **compute at test time**. By giving models more time and more sophisticated tools to think, we unlock new capabilities for tackling problems that were once far beyond their reach. The next generation of AI will not just be more knowledgeable; it will be better at thinking. And as we stand here in July 2025, it feels like we are only just beginning to scratch the surface of what that truly means.

---

### *Footnotes*

 Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., & Zhou, D. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. *arXiv preprint arXiv:2201.11903*.

 Snell, C., Lee, J., Xu, K., & Kumar, A. (2024). Scaling Test-Time Compute Optimally Can be More Effective than Scaling LLM Parameters. *arXiv preprint*.

 Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large Language Models are Zero-Shot Reasoners. *arXiv preprint arXiv:2205.11916*.

 Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., & Zhou, D. (2022). Self-Consistency Improves Chain of Thought Reasoning in Language Models. *arXiv preprint arXiv:2203.11171*.

 Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., & Narasimhan, K. (2023). Tree of Thoughts: Deliberate Problem Solving with Large Language Models. *arXiv preprint arXiv:2305.10601*.

 Besta, M., Blach, N., Kubicek, A., Kyrola, A., Losa, I., Grot, B., & Hofstee, P. (2023). Graph of Thoughts: Solving Elaborate Problems with Large Language Models. *arXiv preprint arXiv:2308.09687*.

 Hao, S., Dai, Z., Liu, Z., Ma, Y., Yang, Z., & Li, Z. (2023). Reasoning with Language Model is Planning with World Model. *arXiv preprint arXiv:2305.14992*.